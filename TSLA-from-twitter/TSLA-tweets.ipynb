{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "!pip install mysql-connector-python\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "import config\n",
    "import tools\n",
    "import mysql.connector\n",
    "import urllib.parse\n",
    "from mysql.connector import errorcode\n",
    "import sqlite3\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definition for BearerTokenAuth, inherits from AuthBase\n",
    "class BearerTokenAuth(AuthBase):\n",
    "    \"\"\"class which handles bearer token requests and authentification\"\"\"\n",
    "    def __init__(self, consumer_key, consumer_secret):\n",
    "        \"\"\"initializes a BearerTokenAuth object\"\"\"\n",
    "        self.bearer_token_url = \"https://api.twitter.com/oauth2/token\"\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.bearer_token = self.get_bearer_token()\n",
    "\n",
    "    def get_bearer_token(self):\n",
    "        \"\"\"requests bearer token\"\"\"\n",
    "        response = requests.post(\n",
    "            self.bearer_token_url,\n",
    "            auth=(self.consumer_key, self.consumer_secret),\n",
    "            data={'grant_type': 'client_credentials'},\n",
    "            headers={'User-Agent': 'LabsRecentSearchQuickStartPython'})\n",
    "\n",
    "        if response.status_code is not 200:\n",
    "            raise Exception(\"Cannot get a Bearer token (HTTP %d): %s\" %\n",
    "                            (response.status_code, response.text))\n",
    "\n",
    "        body = response.json()\n",
    "        return body['access_token']\n",
    "\n",
    "    def __call__(self, r):\n",
    "        \"\"\"sets headers\"\"\"\n",
    "        r.headers['Authorization'] = f\"Bearer %s\" % self.bearer_token\n",
    "        r.headers['User-Agent'] = 'LabsRecentSearchQuickStartPython'\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a mysql connection to aws db\n",
    "def aws_connect(db_name=None):\n",
    "    \"\"\"function which will connect us to aws database instance of our choosing\"\"\"\n",
    "    connection = mysql.connector.connect(\n",
    "    host = config.AWS_ENDPOINT,\n",
    "    user = config.AWS_USER,\n",
    "    passwd = config.AWS_PASSWORD,\n",
    "    port = config.AWS_PORT,\n",
    "    database = db_name)\n",
    "    return connection\n",
    "\n",
    "# create a connection with aws db\n",
    "cnx = aws_connect()\n",
    "\n",
    "# create a cursor over the mysql connection\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# function to create our AWS database\n",
    "def create_database(cursor, database):\n",
    "    \"\"\"creates an aws instance\"\"\"\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"CREATE DATABASE {} DEFAULT CHARACTER SET 'utf8'\".format(database))\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Failed creating database: {}\".format(err))\n",
    "        exit(1)\n",
    "        \n",
    "# name of our database\n",
    "db_name = '$TSLA'\n",
    "\n",
    "# check to see if the database already exists, and if it doesn't, create it\n",
    "try:\n",
    "    cursor.execute(\"USE {}\".format(db_name))\n",
    "except mysql.connector.Error as err:\n",
    "    print(\"Database {} does not exists.\".format(db_name))\n",
    "    if err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "        create_database(cursor, db_name)\n",
    "        print(\"Database {} created successfully.\".format(db_name))\n",
    "        cnx.database = db_name\n",
    "    else:\n",
    "        print(err)\n",
    "        exit(1)\n",
    "        \n",
    "# template for our db tables\n",
    "TABLES = {}\n",
    "TABLES['tweet_time_price2'] = (\n",
    "    \"CREATE TABLE tweet_time_price2 (\"\n",
    "    \"  tweet varchar(250) NOT NULL,\"\n",
    "    \"  id varchar(50) NOT NULL,\"\n",
    "    \"  datetime datetime NOT NULL,\"\n",
    "    \"  price varchar(10) NOT NULL\"\n",
    "    \") ENGINE=InnoDB\")\n",
    "\n",
    "# create tables according to above template\n",
    "for table_name in TABLES:\n",
    "    table_description = TABLES[table_name]\n",
    "    try:\n",
    "        print(\"Creating table {}: \".format(table_name), end='')\n",
    "        cursor.execute(table_description)\n",
    "    except mysql.connector.Error as err:\n",
    "        if err.errno == errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "            print(\"already exists.\")\n",
    "        else:\n",
    "            print(err.msg)\n",
    "    else:\n",
    "        print(\"OK\")\n",
    "\n",
    "# close connections to our cursors\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "# reopen a mysql connection\n",
    "cnx = aws_connect('$TSLA')\n",
    "# recreate a cursor over the mysql connection\n",
    "cursor = cnx.cursor()\n",
    "# insert statement to add a tweet\n",
    "add_tweet = (\"INSERT INTO tweet_time_price2\"\n",
    "               \"(tweet, id, datetime, price)\"\n",
    "               \"VALUES (%s, %s, %s, %s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time intervals over which we'll query our tweets\n",
    "initial_time = '2020-05-20 09:00:00'\n",
    "final_time = '2020-05-22 23:00:00'\n",
    "intervals = list(pd.date_range(initial_time, final_time, freq='T'))\n",
    "intervals1 = [str(i).split() for i in intervals]\n",
    "intervals2 = [i[0]+\"T\"+i[1]+\"Z\" for i in intervals1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant search parameters\n",
    "query = urllib.parse.quote(\"\"\"\"$TSLA\" lang:en\"\"\")\n",
    "max_results = \"100\"\n",
    "headers = {\"Accept-Encoding\": \"gzip\"}\n",
    "\n",
    "# Script starts here\n",
    "for i in range(2694, len(intervals2)-1):\n",
    "    \n",
    "    start = intervals2[i]\n",
    "    end =  intervals2[i+1]\n",
    "    url = f\"https://api.twitter.com/labs/2/tweets/search?query={query}&max_results={max_results}&start_time={start}&end_time={end}\"\n",
    "    \n",
    "    # first page of calls\n",
    "    response = requests.get(url, auth=BearerTokenAuth(config.TWITTER_API_KEY, config.TWITTER_API_SECRET_KEY), headers=headers)\n",
    "    if response.status_code is not 200:\n",
    "        raise Exception(f\"Request returned an error: %s, %s\" % (response.status_code, response.text))\n",
    "    parsed = json.loads(response.text)\n",
    "    pretty_print = json.dumps(parsed, indent=2, sort_keys=True)\n",
    "    print(pretty_print)\n",
    "    # adds our parsed results to our database\n",
    "    try:\n",
    "        for j in parsed['data']:\n",
    "            cursor.execute(add_tweet, (j['text'], j['id'], intervals2[i+1], 'null'))\n",
    "            cnx.commit()\n",
    "    except:\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall our tweets from our aws database\n",
    "cursor.execute(\"\"\"SELECT tweet, id, datetime, price FROM $TSLA.tweet_time_price2 ORDER BY datetime ASC\"\"\")\n",
    "# store the tweets in a dataframe\n",
    "df1 = pd.DataFrame(cursor.fetchall())\n",
    "df1.columns = [x[0] for x in cursor.description]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle our dataframe of tweets to save the work we've done\n",
    "with open(\"pickle-folder/tsla_week2_tweets.pkl\", 'wb') as f:\n",
    "    pickle.dump(df1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload our pickled tweets into our df; this serves as a checkpoint\n",
    "with open(\"pickle-folder/tsla_week2_tweets.pkl\", 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort tweets\n",
    "df1.sort_values(\"tweet\", inplace = True) \n",
    "# drop duplicte values \n",
    "df1.drop_duplicates(subset =\"tweet\", keep = 'first', inplace = True)\n",
    "# resort our de-duped tweets by datetime\n",
    "df1.sort_values(\"datetime\", inplace = True)\n",
    "df1.set_index('datetime',inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a VADER sentiment analyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "# get sentiment scores, store compound sentiment score as our sentiment column\n",
    "df1['sentiment'] = [analyser.polarity_scores(i) for i in df1['tweet']]\n",
    "df1['compound_sentiment'] = [i['compound'] for i in df1['sentiment']]\n",
    "df1['positive_sentiment'] = [i['pos'] for i in df1['sentiment']]\n",
    "df1['negative_sentiment'] = [i['neg'] for i in df1['sentiment']]\n",
    "df1['neutral_sentiment'] = [i['neu'] for i in df1['sentiment']]\n",
    "# drop the column containing sentiment dictionary object\n",
    "df1.drop(columns=['sentiment'], inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual of cyclical tweet volume\n",
    "sns.distplot(df1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our tweets for pickle checkpoint\n",
    "with open(\"pickle-folder/tsla_week2_tweets_df.pkl\", 'wb') as f:\n",
    "    pickle.dump(df1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our tweets for pickle checkpoint\n",
    "with open(\"pickle-folder/tsla_week2_tweets_df.pkl\", 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "with open(\"pickle-folder/tsla_week2_tweets.pkl\", 'wb') as f:\n",
    "    pickle.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
